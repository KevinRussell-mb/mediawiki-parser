Goals
=====
* Possible (to represent MediaWiki syntax)
* Extensible per project (for {for} syntax, DekiWiki conditionals, etc.)

Parser libs
===========
In the following lists, * signifies a pro, "o" a con, and "." a neutral point.

LEPL
----
* Supports ambiguous grammars
o Idiosyncratic syntax with lots of operator overloading (even slices!)
o Slow (http://www.quora.com/What-is-the-best-parser-generator-for-Python/answer/Matthew-Lloyd)

PLY
---
. LALR or, optionally, SLR
* Modal lexer and parser. (Is this necessary? Understand what can't be BNF'd about apostrophe jungles and lists.)
* Easy to translate into C later
. Can turn off magic autodiscovery
o Potential for yacc to guess wrong about how to assemble symbols: reduce/reduce and shift/reduce errors
* Much faster than PyParsing? http://www.mefeedia.com/watch/29412150 at 23:58 suggests it's 5.5x faster. More benchmarks (ANTLR and more): http://www.dalkescientific.com/writings/diary/archive/2007/11/03/antlr_java.html
o A bit more verbose (but very clear)

PyParsing
---------
. Recursive descent (LL)
* Easier to write and debug?
o "[An LL(k) parser] may defer error detection to a different branch of the grammar due to backtracking, often making errors harder to localize across disjunctions with long common prefixes."â€”Wikipedia. I had that problem when writing a simple italics/bold parser: you have to keep the recursion stack in your head to make any sense of the debug info. I eventually gave up trying to fix it.

PyBison
-------
Not researched in depth.
* Claims to be nearly as fast as C
o Requires a C build step

ANTLR
-----
o Separate code generation step
o Slow because it generates a lot of function calls

Previous work
=============
* OCaml lexer implementation: http://www.mediawiki.org/wiki/MediaWiki_lexer
* Markup spec: http://www.mediawiki.org/wiki/Markup_spec
* BNF grammar: http://www.mediawiki.org/wiki/Markup_spec/BNF
  * Corresponds closely to yacc input format
  * Pretty comprehensive: lots of English describing corner cases and error recovery
  . Also discusses render phase
* EBNF grammar: http://www.mediawiki.org/wiki/Markup_spec/EBNF
  * Well-organized and concise
  o Nothing about error recovery
  o Wrong in some places (like the header rules that chew up whitespace)
* flex implementation: http://www.mediawiki.org/wiki/Markup_spec/flex
  o Prints HTML directly; doesn't seem to have a consume/parse/render flow
  o Doesn't seem very comprehensive. I converted it quickly to a PLY lex implementation (fixed the \135 codes and such), and it didn't seem to do a particularly good job recognizing things. There are some heuristics we can glean from it, however, like stripping any trailing comma or period off a scanned URL. Another example is that it doesn't look like it handles the "== H2 ===" case correctly.

Milestones
==========
* Understand what's so hard about apostrophes and lists (http://www.mediawiki.org/wiki/Markup_spec/BNF/Inline_text).
  * This claims MW isn't context-free and has C code on how to hack through the apostrophe jungle: http://web.archiveorange.com/archive/v/e7MXfq0OoW0nCOGyX0oa
  * Useful background discussion by the folks who wrote the BNF attempt: http://www.mediawiki.org/wiki/Talk:Markup_spec
  * The flex markup looks to have naive apostrophe jungle state rules: http://www.mediawiki.org/wiki/Markup_spec/flex
  * mwlib has a pretty clean, decoupled Python impl. See styleanalyzer.py.
* Get a parse tree out of a lib.
* Think about extensibility
* Get apostrophes working (to test ambiguity support).
* Implement productions, tag by tag

Units of estimation
===================
* Apostrophe jungle
* Tables
* Lists
* HTML outputter

Notes
=====
LR good. LALR even better.

If we build the parse tree in custom lexer callbacks, we can make it an ElementTree or whatever we want--meaning we can use XPath on it later if we want.